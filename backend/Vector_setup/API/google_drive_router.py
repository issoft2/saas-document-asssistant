
from fastapi import APIRouter, HTTPException, Depends, status, Request
from sqlmodel import Session
from fastapi.responses import RedirectResponse, JSONResponse
from urllib.parse import parse_qs, urlencode
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from google.oauth2.credentials import Credentials
from Vector_setup.user.db import DBUser, TenantGoogleDriveConfig, get_db
from Vector_setup.API.admin_permission import require_tenant_admin
from Vector_setup.user.auth_jwt import get_current_user
import os
from pydantic import BaseModel
from typing import List, Optional
from googleapiclient.http import MediaIoBaseDownload
from io import BytesIO
import uuid
from Vector_setup.base.db_setup_management import MultiTenantChromaStoreManager
from Vector_setup.API.ingest_routes import extract_text_from_upload

# Single shared store instance

import logging

logger = logging.getLogger(__name__)

from dotenv import load_dotenv


load_dotenv()

# Get the environment variables
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
GOOGLE_REDIRECT_URI = os.getenv("GOOGLE_REDIRECT_URI")
GOOGLE_SCOPES = os.getenv("GOOGLE_SCOPES", "")
FRONTEND_AFTER_CONNECT_URL = os.getenv("FRONTEND_AFTER_CONNECT_URL")


router = APIRouter(prefix="/google-drive", tags=["google-drive"])

@router.get("/auth-url")
def get_google_drive_auth_url(
    current_user: DBUser = Depends(require_tenant_admin),
):
    tenant_id = current_user.tenant_id

    # Put tenant_id (and maybe user id) into state so callback knows where to store tokens
    state_data = {"tenant_id": tenant_id}
    state = urlencode(state_data)

    flow = Flow.from_client_config(
        {
            "web": {
                "client_id": GOOGLE_CLIENT_ID,
                "client_secret": GOOGLE_CLIENT_SECRET,
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token",
                "redirect_uris": [GOOGLE_REDIRECT_URI],
            }
        },
        scopes=GOOGLE_SCOPES,
    )
    flow.redirect_uri = GOOGLE_REDIRECT_URI

    authorization_url, _ = flow.authorization_url(
        access_type="offline",
        include_granted_scopes="true",
        state=state,
        prompt="consent",  # ensures refresh_token is returned
    )

    return JSONResponse({"auth_url": authorization_url})


@router.get("/callback")
def google_drive_callback(
    request: Request,
    code: str,
    state: str,
    db: Session = Depends(get_db),
):
    # 1) Parse tenant_id from state
    state_params = parse_qs(state)
    tenant_ids = state_params.get("tenant_id")
    if not tenant_ids:
        raise HTTPException(status_code=400, detail="Missing tenant in state")
    tenant_id = tenant_ids[0]

    # 2) Build OAuth flow
    flow = Flow.from_client_config(
        {
            "web": {
                "client_id": GOOGLE_CLIENT_ID,
                "client_secret": GOOGLE_CLIENT_SECRET,
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token",
                "redirect_uris": [GOOGLE_REDIRECT_URI],
            }
        },
        scopes=GOOGLE_SCOPES.split(),  # if you stored as space-separated string
    )
    flow.redirect_uri = GOOGLE_REDIRECT_URI

    # 3) Exchange code for tokens
    authorization_response = str(request.url)
    flow.fetch_token(authorization_response=authorization_response)
    creds = flow.credentials
    if not creds.refresh_token:
        raise HTTPException(status_code=400, detail="No refresh token from Google")

    # 4) Get account email
    drive_service = build("drive", "v3", credentials=creds)
    about = drive_service.about().get(fields="user(emailAddress)").execute()
    account_email = about["user"]["emailAddress"]

    # 5) Store per tenant
    cfg = (
        db.query(TenantGoogleDriveConfig)
        .filter_by(tenant_id=tenant_id)
        .first()
    )
    if not cfg:
        cfg = TenantGoogleDriveConfig(
            tenant_id=tenant_id,
            refresh_token=creds.refresh_token,
            account_email=account_email,
        )
        db.add(cfg)
    else:
        cfg.refresh_token = creds.refresh_token
        cfg.account_email = account_email
    print("Saving Google Drive config for tenant", tenant_id, account_email)
    db.commit()
    print("Saved config")

    return RedirectResponse(FRONTEND_AFTER_CONNECT_URL)



@router.get("/status")
def google_drive_status(
    current_user: DBUser = Depends(require_tenant_admin),
    db: Session = Depends(get_db),
):
    cfg = (
        db.query(TenantGoogleDriveConfig)
        .filter_by(tenant_id=current_user.tenant_id)
        .first()
    )
    if not cfg:
        return {"connected": False, "account_email": None}
    return {"connected": True, "account_email": cfg.account_email}


# Added a helper that reconstructs credentials for the current tenant
def get_drive_service_for_tenant(
    tenant_id: str,
    db: Session
):
    cfg = (
        db.query(TenantGoogleDriveConfig)
        .filter_by(tenant_id=tenant_id)
        .first()
    )
    if not cfg:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Google Drive is not Connected for this tenant.",
            )
        
    scopes = GOOGLE_SCOPES.split() if GOOGLE_SCOPES else [
        "https://www.googleapis.com/auth/drive.readonly"
    ]
    
    creds = Credentials(
        token=None, # Will be obtained using refresh_token
        refresh_token=cfg.refresh_token,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=GOOGLE_CLIENT_ID,
        client_secret=GOOGLE_CLIENT_SECRET,
        scopes=scopes
    )
    
    try:
       service = build("drive", "v3", credentials=creds)   
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to initialize Google Drive client: {e}",
        )
        
    return service


# Endpoint List files from Drive
class DriveFileOut(BaseModel):
    id: str
    name: str
    mime_type: str

GOOGLE_FOLDER_MIME = "application/vn.google-apps.folder"    

@router.get("/files", response_model=List[DriveFileOut])
def list_drive_files(
    folder_id: Optional[str] = None,
    page_size: int = 100,
    recursive: bool = False,
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(require_tenant_admin),
    
):
    """
    
    List files in this tenant's connected Google Drive.
    If folder_id is provided, list that folder's children.
    - Otherwise, list items in 'My Drive' visible to this account.
    - If recursive=True, tranverse subfolders and return all decendants.
    """
    tenant_id = current_user.tenant_id
    service = get_drive_service_for_tenant(tenant_id, db)
    
    def _list_page(q: str) -> list[dict]:
        # you can also set spaces="drive" explicitly if needed.
        results = (
            service.files()
            .list(
                q=q,
                pageSize=page_size,
                fields="nextPageToken, files(id, name, mimeType, parents)",
            )
            .execute()
        )
        return results.get("files", [])
    
    # Base query: non-trashed files.
    q_parts = ["trashed = false"]
    if folder_id:
        q_parts.append(f"'{folder_id}' in parents")
    #else: omit parents filter to treat as root/visible.
        
        
    query = " and ".join(q_parts)
    
    if not recursive:
        files = _list_page(query)
    else:
        # BFS/DFS through subfolders using multiple list calls.
        files: list[dict] = []
        folders_to_visit = [folder_id] if folder_id else [None]
        
        visited_folders: set[str] = set()
        
        while folders_to_visit:
            current_folder = folders_to_visit.pop()
            q_parts = ["trashed = false"]
            if current_folder:
                q_parts.append(f"'{current_folder}' in parents")
                
            query = " and ".join(q_parts)
            
            page_files = _list_page(query)
            files.extend(page_files)
            
            # Add any child folders to the queue
            for f in page_files:
                if f.get("mimeType") == GOOGLE_FOLDER_MIME:
                    fid = f["id"]
                    if fid not in visited_folders:
                        visited_folders.add(fid)
                        folders_to_visit.append(fid)
                                
    # Map raw Drive items to your schema, including mime_type.
    return [
        DriveFileOut(
            id=f["id"],
            name=f["name"],
            mime_type=f.get("mimeType", ""),
            # Optional: parent_id=f.get("parents", [None])[0] if you want
        )
        for f in files
        if f.get("mimeType") != GOOGLE_FOLDER_MIME # exclude folders themselves
    ]
    
    
 # Single shared store instance
vector_store = MultiTenantChromaStoreManager("./chromadb_multi_tenant")
   
def get_store() -> MultiTenantChromaStoreManager:
    return vector_store


class DriveIngestRequest(BaseModel):
    file_id: str
    collection_name: str
    title: Optional[str] = None



# refine ingest function for google native documents supports
GOOGLE_DOC_MIME = "application/vnd.google-apps.document"
GOOGLE_SHEET_MIME = "application/vnd.google-apps.spreadsheet"
GOOGLE_SLIDE_MIME = "application/vnd.google-apps.presentation"

@router.post("/ingest")
async def ingest_drive_file(
    req: DriveIngestRequest,
    db: Session = Depends(get_db),
    store: MultiTenantChromaStoreManager = Depends(get_store),
    current_user: DBUser = Depends(require_tenant_admin),
):
    """
    Download a file from Google Drive for this tenant, and ingest it into collection.
    
    Supports:
    - Binary files (pdf, docx, txt, md, xlsx, etc.).
    - Google Docs/Sheet/Slides via export.
    """
    
    tenant_id = current_user.tenant_id
    service = get_drive_service_for_tenant(tenant_id=tenant_id, db=db)
    
    # Get file metadata
    file_meta = (
        service.files()
        .get(field=req.file_id, fields="id, name, mimeType")
        .execute()
    )
    original_name = file_meta["name"]
    mime_type = file_meta.get("mimeType", "")
    
    buf = BytesIO()
    
    # Decide how to download/export and what "filename" to pass to extractor
    if mime_type.startswith("application/vnd.google-apps"):
        # Google Workspace files: choose export target per type
        if mime_type == GOOGLE_DOC_MIME:
            export_mime = "application/pdf"
            synthetic_filename = f"{original_name}.pdf"
        elif mime_type == GOOGLE_SHEET_MIME:
            # if your pipeline handles excel with pandas/openyxl
            export_mime = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            synthetic_filename = f"{original_name}.xlsx"
            
        elif mime_type == GOOGLE_SLIDE_MIME:
            # Download as PDF
            export_mime = "application/pdf"
            synthetic_filename = f"{original_name}.pdf"
            
        else:
            # Fallback: try PDF
            export_mime = "application/pdf"
            synthetic_filename = f"{original_name}.pdf"
            
        request = service.files().export_media(
            fileId=req.file_id, mimeType=export_mime
        )
    else:
        # Normal Drive binary files (PDF, DOCX, etc.)
        synthetic_filename = original_name # already has extenation
        request = service.files().get_media(fileId=req.file_id)
        
    downloader = MediaIoBaseDownload(buf, request)
    done = False
    while not done:
        status_chunk, done = downloader.next_chunk()
        #log(status_chunk.progress())
        logger.warning("Download %d%%", int(status_chunk.progress() * 100))
        
    raw_bytes = buf.getvalue()
    
    # Run through the existing extraction pipeline using synthetic filename
    text = extract_text_from_upload(synthetic_filename, raw_bytes)
    if not isinstance(text, str) or not text.strip():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="No text could be extracted from the GOOGLE Drive file.",
        )
        
    doc_id = str(uuid.uuid4())
    metadata = {
        "filename": original_name,
        "title": req.title or original_name,
        "content_type": mime_type,
        "source": "google_drive",
        "drive_file_id": req.file_id,
        "tenant_id": tenant_id,
        "collection": req.collection_name,
    }
    
    result = await store.add_document(
        tenant_id=tenant_id,
        collection_name=req.collection_name,
        doc_id=doc_id,
        text=text,
        metadata=metadata
    )
    
    if result.get("status") != "ok":
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=request.get("message", "Indexing failed!"),
        )
        
    return {"status": "ok", "doc_id": doc_id}                        
            
                
            



# @router.post("/ingest")
# async def ingest_drive_file(
#     req: DriveIngestRequest,
#     db: Session = Depends(get_db),
#     store: MultiTenantChromaStoreManager = Depends(get_store),
#     current_user: DBUser = Depends(require_tenant_admin),
# ):
#     """
#     Download a file from Google Drive for this tenant and ingest it into a collection.
#     """
#     tenant_id = current_user.tenant_id
#     service = get_drive_service_for_tenant(tenant_id, db)

#     # 1) Get file metadata
#     file_meta = (
#         service.files()
#         .get(fileId=req.file_id, fields="id, name, mimeType")
#         .execute()
#     )
#     filename = file_meta["name"]
#     mime_type = file_meta.get("mimeType", "")

#     # 2) Download or export contents
#     buf = BytesIO()
#     if mime_type.startswith("application/vnd.google-apps"):
#         # Google Docs/Sheets/Slides/etc â†’ export to PDF
#         export_mime = "application/pdf"
#         request = service.files().export_media(
#             fileId=req.file_id, mimeType=export_mime
#         )
#     else:
#         # Normal Drive binary files (PDF, DOCX, etc.)
#         request = service.files().get_media(fileId=req.file_id)

#     downloader = MediaIoBaseDownload(buf, request)
#     done = False
#     while not done:
#         status_chunk, done = downloader.next_chunk()
#         # optional: log status_chunk.progress()

#     raw_bytes = buf.getvalue()

#     # 3) Run through your existing extraction pipeline
#     text = extract_text_from_upload(filename, raw_bytes)
#     if not isinstance(text, str) or not text.strip():
#         raise HTTPException(
#             status_code=status.HTTP_400_BAD_REQUEST,
#             detail="No text could be extracted from the Google Drive file.",
#         )

#     doc_id = str(uuid.uuid4())
#     metadata = {
#         "filename": filename,
#         "title": req.title or filename,
#         "content_type": mime_type,
#         "source": "google_drive",
#         "drive_file_id": req.file_id,
#         "tenant_id": tenant_id,
#         "collection": req.collection_name,
#     }

#     result = await store.add_document(
#         tenant_id=tenant_id,
#         collection_name=req.collection_name,
#         doc_id=doc_id,
#         text=text,
#         metadata=metadata,
#     )

#     if result.get("status") != "ok":
#         raise HTTPException(
#             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
#             detail=result.get("message", "Indexing failed"),
#         )

#     return {"status": "ok", "doc_id": doc_id}


           
             
             
             
             